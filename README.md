---
title: Restaurant QA Chatbot
emoji: ğŸ½ï¸
colorFrom: blue
colorTo: gray
sdk: streamlit
sdk_version: 1.44.1
app_file: app/app.py
pinned: false
license: mit
short_description: Intelligent RAG-based chatbot for restaurants
---

# ğŸ½ï¸ Chatbot Restaurant Q&A

Un chatbot intelligent basÃ© sur l'approche RAG (Retrieval-Augmented Generation), conÃ§u pour rÃ©pondre automatiquement aux questions frÃ©quentes concernant un restaurant. Il extrait les rÃ©ponses directement depuis un fichier PDF structurÃ© contenant les questions/rÃ©ponses officielles.

![App UI](App_UI.png)

---

## ğŸš€ Technologies utilisÃ©es

- **LLM** : Mistral (via [Ollama](https://ollama.com/))
- **Embeddings** : Nomic Embed Text (via Ollama)
- **Vector Store** : Pinecone
- **Framework RAG** : LangChain
- **Interface utilisateur** : Streamlit

---

## âš™ï¸ PrÃ©requis

- Python 3.8 ou supÃ©rieur
- [Ollama](https://ollama.com/) installÃ© localement avec les modÃ¨les suivants :
  - `mistral`
  - `nomic-embed-text`
- Un compte [Pinecone](https://www.pinecone.io/) avec une clÃ© API valide

---

## ğŸ› ï¸ Installation

1. **Cloner le dÃ©pÃ´t** :

```bash
git clone https://github.com/MamadouKane/restaurant-QA-Chatbot.git
cd restaurant-QA-Chatbot
```

2. **Installer les dÃ©pendances** :

```bash
pip install -r requirements.txt
```

3. **Configurer les variables dâ€™environnement** :

CrÃ©ez un fichier `.env` Ã  partir de `.env.example` :

```bash
cp .env.example .env
```

Remplissez-le avec vos informations Pinecone :

```
PINECONE_API_KEY=your_pinecone_api_key
PINECONE_ENVIRONMENT=your_environment
PINECONE_INDEX_NAME=restaurant-qa
```

---

## ğŸ“„ PrÃ©paration des donnÃ©es

### 1. CrÃ©er lâ€™index Pinecone

Avant toute ingestion, vous devez crÃ©er lâ€™index dans Pinecone :

```bash
python app/create_index.py
```

> âš ï¸ Ce script supprimera lâ€™index existant sâ€™il est dÃ©jÃ  prÃ©sent puis la recrÃ©e.

### 2. IngÃ©rer les donnÃ©es Q&R du PDF

Placez votre fichier PDF dans le dossier `data/` puis lancez :

```bash
python app/ingest.py
```

Le script :

- lit tous les fichiers PDF dans `data/`
- extrait chaque question-rÃ©ponse avec la section associÃ©e
- gÃ©nÃ¨re les embeddings
- stocke le tout dans Pinecone

---

## ğŸ’¬ Lancer le chatbot

DÃ©marrez lâ€™interface utilisateur via Streamlit :

```bash
streamlit run app/app.py
```

L'application sera disponible sur : [http://localhost:8501](http://localhost:8501)

---

## âœ¨ FonctionnalitÃ©s

- âœ… Interface intuitive pour poser vos questions
- âœ… Moteur RAG avec rÃ©cupÃ©ration contextuelle des rÃ©ponses
- âœ… RÃ©ponses gÃ©nÃ©rÃ©es Ã  partir du contenu PDF du restaurant
- âœ… MÃ©moire de conversation (historique limitÃ©)
- âœ… RÃ©ponses affichÃ©es en streaming pour plus de fluiditÃ©

---

## âš™ï¸ Comportement technique

- ğŸ” Le retriever retourne les 2 documents les plus pertinents (`k=2`)
- ğŸ§  Le modÃ¨le LLM utilisÃ© est `mistral` (rapide et efficace)
- ğŸ§  La mÃ©moire de conversation est limitÃ©e Ã  quelques Ã©changes pour rester pertinent
- ğŸ§  Les embeddings sont gÃ©nÃ©rÃ©s avec `nomic-embed-text`

---

## ğŸ—‚ï¸ Structure du projet

```
restaurant-QA-Chatbot/
â”œâ”€â”€ app/
â”‚   â”œâ”€â”€ app.py           # Interface Streamlit principale
â”‚   â”œâ”€â”€ ingest.py        # Script d'ingestion des PDF dans Pinecone
â”‚   â”œâ”€â”€ create_index.py  # CrÃ©ation de l'index Pinecone
â”œâ”€â”€ data/                # Contient vos fichiers PDF Ã  ingÃ©rer
â”œâ”€â”€ test.ipynb           # Notebook de test (Colab-friendly)
â”œâ”€â”€ .env                 # Fichier de configuration local
â”œâ”€â”€ .env.example         # Template pour .env
â”œâ”€â”€ requirements.txt     # DÃ©pendances Python
â””â”€â”€ README.md            # Ce fichier
```

---

## ğŸ§ª Tester sur Colab (optionnel)

Un notebook `test.ipynb` est fourni pour faire des tests rapidement, y compris l'ingestion de fichiers et l'interrogation simple sans interface.

---

Built with â¤ï¸ by [Mamadou KANE](https://www.linkedin.com/in/kanemamadou/)
